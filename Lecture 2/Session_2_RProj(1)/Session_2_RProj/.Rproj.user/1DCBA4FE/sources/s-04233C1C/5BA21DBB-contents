###########################################
## Chapter 7 - k-Nearest Neighbors (kNN) ## 
###########################################

#NOTE: Prepared with R version 3.6.0

#set the working directory to appropriate folder on your machine, so as to 
#access the data files.

#load the required libraries/packages for this chapter
#Install the package(s) below once on your machine. To do so, uncomment the 
#install.packages line(s) below.

#install.packages("class")
#install.packages("e1071")
#install.packages("caret")
library(class)
library(caret)
library(e1071)

## Problem 7.2 Personal Loan Acceptance. 
##Universal Bank is a relatively young bank growing rapidly in terms of overall 
##customer acquisition. The majority of these customers are liability customers
##(depositors) with varying sizes of relationship with the bank. The customer 
##base of asset customers (borrowers) is quite small, and the bank is interested
##in expanding this base rapidly to bring in more loan business. In particular,
##it wants to explore ways of converting its liability customers to personal 
##loan customers (while retaining them as depositors).

##A campaign that the bank ran last year for liability customers showed a 
##healthy conversion rate of over 9% success. This has encouraged the retail 
##marketing department to devise smarter campaigns with better target marketing. 
##The goal is to use k-NN to predict whether a new customer will accept a loan 
##offer. This will serve as the basis for the design of a new campaign. 

##The file UniversalBank.csv contains data on 5000 customers. The data include 
##customer demographic information (age, income, etc.), the customer's 
##relationship with the bank (mortgage, securities account, etc.), and the 
##customer response to the last personal loan
##campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) 
##accepted the personal loan that was offered to them in the earlier campaign.
##Partition the data into training (60%) and validation (40%) sets. 

#load the data
universal.df <- read.csv("UniversalBank.csv")
dim(universal.df)
t(t(names(universal.df)))

#partition the data
set.seed(1)  
train.index <- sample(row.names(universal.df), 0.6*dim(universal.df)[1])
valid.index <- setdiff(row.names(universal.df), train.index)  
train.df <- universal.df[train.index, -c(1, 5)]
valid.df <- universal.df[valid.index, -c(1, 5)]
t(t(names(train.df)))

## 7.2.a Consider the following customer:
##Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education = 2, 
##Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1, and
##Credit Card = 1. 
##Perform a k-NN classification with all predictors except ID and ZIP code using
##k = 1. 
##Note that using KNN in the class library, categorical predictors are 
##automatically handled.  If you use FNN, you need to handle categorical 
##variables separately as follows:
##> universal.df$Education <- as.factor(universal.df$Education)
##Use the default cutoff value of 0.5. 
##How would this customer be classified?

new.cust <- data.frame(Age = 40,                
                       Experience = 10,     
                       Income = 84,   
                       Family = 2,          
                       CCAvg = 2,          
                       Education = 2,        
                       Mortgage = 0,           
                       Securities.Account = 0, 
                       CD.Account = 0, 
                       Online = 1,            
                       CreditCard = 1)

#new.cust$Education <- as.factor(new.cust$Education)

# normalize the data
train.norm.df <- train.df[,-8]
valid.norm.df <- valid.df[,-8]

new.cust.norm <- new.cust
norm.values <- preProcess(train.df[, -8], method=c("center", "scale"))
train.norm.df <- predict(norm.values, train.df[, -8])
valid.norm.df <- predict(norm.values, valid.df[, -8])
new.cust.norm <- predict(norm.values, new.cust.norm)


knn.pred <- class::knn(train = train.norm.df, 
                       test = new.cust.norm, 
                       cl = train.df$Personal.Loan, k = 1)
knn.pred
#> knn.pred
#[1] 0
#Levels: 0 1

#From the output we conclude that the above customer is classified as belonging
#to the "loan not accepted" group.

## 7.2.b What is a choice of k that balances between overfitting and ignoring 
##the predictor information?

library(e1071)
# optimal k
accuracy.df <- data.frame(k = seq(1, 15, 1), overallaccurace = rep(0, 15))
for(i in 1:15) {
  knn.pred <- class::knn(train = train.norm.df, 
                         test = valid.norm.df, 
                         cl = train.df$Personal.Loan, k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, 
                                       as.factor(valid.df$Personal.Loan))$overall[1]
}

which(accuracy.df[,2] == max(accuracy.df[,2])) 
#> which(accuracy.df[,2] == max(accuracy.df[,2]))
#[1] 3

accuracy.df
#> accuracy.df
#    k overallaccurace
#1   1          0.9560
#2   2          0.9490
#3   3          0.9585
#4   4          0.9540
#5   5          0.9550
#6   6          0.9540
#7   7          0.9540
#8   8          0.9540
#9   9          0.9540
#10 10          0.9495
#11 11          0.9505
#12 12          0.9490
#13 13          0.9475
#14 14          0.9470
#15 15          0.9465

# best k = 3. The value of k that balances between overfitting (k too small) 
#and ignoring the predictor information (k too large) is 3.

## 7.2.c Show the confusion matrix for the validation data that results from 
##using the best k.

#knn with k = 3
knn.pred <- class::knn(train = train.norm.df, 
                       test = valid.norm.df, 
                       cl = train.df$Personal.Loan, k = 3)
confusionMatrix(knn.pred, as.factor(valid.df$Personal.Loan), positive = "1")
#> confusionMatrix(knn.pred, as.factor(valid.df$Personal.Loan), positive = "1")
#Confusion Matrix and Statistics
#
#           Reference
# Prediction    0    1
#          0 1782   70
#          1   13  135
#
#               Accuracy : 0.9585          
#                 95% CI : (0.9488, 0.9668)
#    No Information Rate : 0.8975          
#    P-Value [Acc > NIR] : < 2.2e-16       
#
#                  Kappa : 0.7428          
#
# Mcnemar's Test P-Value : 7.906e-10       
#
#            Sensitivity : 0.6585          
#            Specificity : 0.9928          
#         Pos Pred Value : 0.9122          
#         Neg Pred Value : 0.9622          
#             Prevalence : 0.1025          
#         Detection Rate : 0.0675          
#   Detection Prevalence : 0.0740          
#      Balanced Accuracy : 0.8256          
#
#       'Positive' Class : 1
#

## 7.2.d. Consider the following customer: Age = 40, Experience = 10, 
##Income = 84, Family = 2, CCAvg = 2, Education = 2, Mortgage = 0, 
##Securities Account = 0, CD Account = 0, Online = 1 and Credit Card = 1. 
##Classify the customer using the best k. 

# predict new customer with k = 3
knn.pred <- class::knn(train = train.norm.df, 
                       test = new.cust.norm, 
                       cl = train.df$Personal.Loan, k = 3)
knn.pred
#> knn.pred
#[1] 0
#Levels: 0 1

## 7.2.e Repartition the data, this time into training, validation, and test 
##sets (50% : 30% : 20%). Apply the k-NN method with the k chosen above. 
##Compare the confusion matrix of the test set with that of the training and 
##validation sets. Comment on the differences and their reason. 

# 3-way partition
set.seed(1)  
train.index <- sample(row.names(universal.df), 0.5*dim(universal.df)[1])
valid.index <- sample(setdiff(row.names(universal.df), train.index), 
                      0.3*dim(universal.df)[1])
test.index <-  setdiff(row.names(universal.df), c(train.index, valid.index)) 
train.df <- universal.df[train.index, -c(1, 5)]
valid.df <- universal.df[valid.index, -c(1, 5)]
test.df <- universal.df[test.index, -c(1, 5)]

# normalization
train.norm.df <- train.df[,-8]
valid.norm.df <- valid.df[,-8]
test.norm.df <- test.df[,-8]
norm.values <- preProcess(train.df[, -8], method=c("center", "scale"))
train.norm.df <- predict(norm.values, train.df[, -8])
valid.norm.df <- predict(norm.values, valid.df[, -8])
test.norm.df <- predict(norm.values, test.df[, -8])

# predictions on train
knn.predt <- class::knn(train = train.norm.df, 
                       test = train.norm.df, 
                       cl = train.df$Personal.Loan, k = 3)

confusionMatrix(knn.predt, as.factor(train.df$Personal.Loan), positive = "1")
#> confusionMatrix(knn.predt, as.factor(train.df$Personal.Loan), positive = "1")
#Confusion Matrix and Statistics
#
#          Reference
#Prediction    0    1
#         0 2259   63
#         1    9  169
#
#               Accuracy : 0.9712          
#                 95% CI : (0.9639, 0.9774)
#    No Information Rate : 0.9072          
#    P-Value [Acc > NIR] : < 2.2e-16       
#
#                  Kappa : 0.809           
#
# Mcnemar's Test P-Value : 4.208e-10       
#
#            Sensitivity : 0.7284          
#            Specificity : 0.9960          
#         Pos Pred Value : 0.9494          
#         Neg Pred Value : 0.9729          
#             Prevalence : 0.0928          
#         Detection Rate : 0.0676          
#   Detection Prevalence : 0.0712          
#      Balanced Accuracy : 0.8622          
#
#       'Positive' Class : 1

# predictions on validation
knn.predv <- class::knn(train = train.norm.df, 
                       test = valid.norm.df, 
                       cl = train.df$Personal.Loan, k = 3)

confusionMatrix(knn.predv, as.factor(valid.df$Personal.Loan), positive = "1")
#> confusionMatrix(knn.predv, as.factor(valid.df$Personal.Loan), positive = "1")
#Confusion Matrix and Statistics
#
#           Reference
# Prediction    0    1
#          0 1357   47
#          1    7   89
#
#                Accuracy : 0.964           
#                  95% CI : (0.9533, 0.9728)
#     No Information Rate : 0.9093          
#     P-Value [Acc > NIR] : < 2.2e-16       
#
#                   Kappa : 0.7484          
#
#  Mcnemar's Test P-Value : 1.113e-07       
#                                          
#             Sensitivity : 0.65441         
#             Specificity : 0.99487         
#          Pos Pred Value : 0.92708         
#          Neg Pred Value : 0.96652         
#              Prevalence : 0.09067         
#          Detection Rate : 0.05933         
#    Detection Prevalence : 0.06400         
#       Balanced Accuracy : 0.82464         
#                                          
#        'Positive' Class : 1

# predictions on test
knn.predtt <- class::knn(train = train.norm.df, 
                       test = test.norm.df, 
                       cl = train.df$Personal.Loan, k = 3)
confusionMatrix(knn.predtt, as.factor(test.df$Personal.Loan), positive = "1")
#> confusionMatrix(knn.predtt, as.factor(test.df$Personal.Loan), positive = "1")
#Confusion Matrix and Statistics
#
#           Reference
# Prediction   0   1
#          0 882  39
#          1   6  73
#
#                Accuracy : 0.955          
#                  95% CI : (0.9402, 0.967)
#     No Information Rate : 0.888          
#     P-Value [Acc > NIR] : 4.309e-14      
#
#                   Kappa : 0.7403         
#
#  Mcnemar's Test P-Value : 1.840e-06      
#                                         
#             Sensitivity : 0.6518         
#             Specificity : 0.9932         
#          Pos Pred Value : 0.9241         
#          Neg Pred Value : 0.9577         
#              Prevalence : 0.1120         
#          Detection Rate : 0.0730         
#    Detection Prevalence : 0.0790         
#       Balanced Accuracy : 0.8225         
#                                         
#        'Positive' Class : 1

#We choose the best k, which minimizes the misclassification rate in the 
#validation set. Our best k is 3. From the above confusion matrices we observe
#the following:

#The error rate increases from the training set to the validation set, and again
#from the validation set to the test set.  The differences are small, but this
#decreased performance, at least in the test set, is not unexpected - both the 
#training and validation sets are used in setting the optimal k so there can be 
#overfitting.  The test set was not used to select the optimal k, so reflects
#expected performance with new data, slightly less accurate.

#############################

## Problem 7.3 Predicting Housing Median Prices. 
##The file BostonHousing.csv contains information on over 500 census tracts in 
##Boston, where for each tract multiple variables are recorded. The last column
##(CAT.MEDV) was derived from MEDV, such that it obtains the value 1 if 
##MEDV > 30 and 0 otherwise. Consider the goal of predicting the median value 
##(MEDV) of a tract, given the information in the first 12 columns. 
##Partition the data into training (60%) and validation (40%) sets. 

#load the data
housing.df <- read.csv("BostonHousing.csv")
t(t(names(housing.df)))
#partition into training and validation sets
set.seed(1)  
train.index <- sample(row.names(housing.df), 0.6*dim(housing.df)[1])  
valid.index <- setdiff(row.names(housing.df), train.index)  
train.df <- housing.df[train.index, -14]
valid.df <- housing.df[valid.index, -14]

train.norm.df <- train.df
valid.norm.df <- valid.df
norm.values <- preProcess(train.df[, -13], method=c("center", "scale"))
train.norm.df[, -13] <- predict(norm.values, train.df[, -13])
valid.norm.df[, -13] <- predict(norm.values, valid.df[, -13])

## 7.3.a Perform a k-NN prediction with all 12 predictors (ignore the CAT.MEDV
##column), trying values of k from 1 to 5. Make sure to normalize the data, and
##choose function knn() from package class rather than package FNN. To make sure
##R is using the class package (when both packages are loaded), use class::knn(). 
##What is the best k? What does it mean? 

library(caret)
accuracy.df <- data.frame(k = seq(1, 5, 1), RMSE = rep(0, 5))
for(i in 1:5) {
  knn.pred <- class::knn(train = train.norm.df[,-13], 
                         test = valid.norm.df[,-13], 
                         cl = train.df$MEDV, k = i)
  accuracy.df[i, 2] <- RMSE(as.numeric(as.character(knn.pred)), valid.df$MEDV)
}                
accuracy.df
#> accuracy.df
#  k     RMSE
#1 1 5.293825
#2 2 5.810933
#3 3 6.236556
#4 4 7.353411
#5 5 6.963968

#Here best k = 1. However, we will not use k=1 (reminder). The next lowest error
#is for k=2. This means that, for a given record, MEDV is predicted by 
#averaging the MEDVs for the 2 closest records, proximity being measured by the
#distance between the vectors of predictor values.

## 7.3.b Predict the MEDV for a tract with the following information, using the 
##best k:
##CRIM  ZN INDUS  CHAS   NOX RM AGE DIS RAD TAX PTRATIO LSTAT
## 0.2   0     7     0 0.538  6  62 4.7   4 307      21    10

new.rec <- data.frame(0.2, 0, 7, 0, 0.538, 6, 62, 4.7, 4, 307, 21, 10)
names(new.rec) <- names(train.norm.df)[-13]
new.norm.rec <- predict(norm.values, new.rec)

knn.pred <- class::knn(train = train.norm.df[,-13], 
                       test = new.norm.rec, 
                       cl = train.df$MEDV, k = 2)
knn.pred
#> knn.pred
#[1] 20.4
#173 Levels: 5 5.6 7 7.2 7.4 7.5 8.1 8.3 8.5 8.7 8.8 9.6 9.7 10.2 10.4 10.5 10.8 11 11.3 ... 50

## 7.3.c If we used the above k-NN algorithm to score the training data, what 
##would be the error of the training set?

#In the training set, the error is zero because the training cases are matched 
#to themselves. 

## 7.3.d Why is the validation data error overly optimistic compared to the 
##error rate when applying this k-NN predictor to new data?

#The validation error measures the error for the "best k" among multiple k's 
#tried out for the validation data, so that particular k is optimized for 
#the particular validation data set that was used in selecting it. It may 
#not be as suitable for the new data.

## 7.3.e If the purpose is to predict MEDV for several thousands of new tracts, what would 
##be the disadvantage of using k-NN prediction? List the operations that the algorithm goes 
##through in order to produce each prediction. 

#KNN does not yield a uniform rule that can be applied to each new record to be
#predicted -- the whole "model building" process has to be repeated for each new
#record to be classified.

#Specifically, the algorithm must calculate the distance from a new record to 
#each of the training records, select the n-closest training records, determine
#the average target value for the n-closest training records, then score that 
#target value to the new record, then repeat this process for each of the new 
#records.

#Note that the computations required are a function both of the size of the 
#training set and the size of the new data to be scored. 
