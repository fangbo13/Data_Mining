#####################################################
## Chapter 2 - Overview of the Data Mining Process ## 
##################################################### 

#NOTE: Prepared with R version 3.5.2.

#set the working directory to appropriate folder on your machine, so as to access 
#the data files.


##Problem 2.1 Assuming that data mining techniques are to be used in the 
##following cases, identify whether the task required is supervised or 
##unsupervised learning.

##2.1.a. Deciding whether to issue a loan to an applicant based on demographic 
##and financial data (with reference to a database of similar data on prior 
##customers).

#This is supervised learning, because the database includes information on 
#whether the loan was approved or not.

##2.1.b. In an online bookstore, making recommendations to customers concerning 
##additional items to buy based on the buying patterns in prior transactions.

#This is unsupervised learning, because there is no apparent outcome 
#(e.g., whether the recommendation was adopted or not).

##2.1.c. Identifying a network data packet as dangerous (virus, hacker attack) 
##based on comparison to other packets whose threat status is known.

#This is supervised learning, because for the other packets the status is known.

##2.1.d. Identifying segments of similar customers.

#This is unsupervised learning because there is no known outcome (though once 
#you use unsupervised learning to identify segments, you could use supervised 
#learning to classify new customers into those segments).

##2.1.e. Predicting whether a company will go bankrupt based on comparing its 
##financial data to those of similar bankrupt and nonbankrupt firms.

#This is supervised learning, because the status of the similar firms is known.

##2.1.f. Estimating the repair time required for an aircraft based on a trouble
##ticket.

#This is supervised learning, because there is likely to be knowledge of actual
#(historic) repair times of similar repairs.

##2.1.g. Automated sorting of mail by zip code scanning.

#This is supervised learning, as there is likely to be knowledge about whether
#the sorting was correct in previous mail sorting.

##2.1.h. Printing of custom discount coupons at the conclusion of a grocery store
##checkout based on what you just bought and what others have bought previously.

#This is unsupervised learning, if we assume that we do not know what will be 
#purchased in the future.

#############################

##Problem 2.2 Describe the difference in roles assumed by the validation 
##partition and the test partition.

#The validation partition is used to assess the performance of each supervised
#learning model so that we can compare models and pick the best one. In some 
#algorithms (e.g., classification and regression trees, k-nearest neighbors) 
#the validation partition may be used in automated fashion to tune and improve 
#the model. This means that the validation data are actually used to help build
#the model.

#The test data partition is used for assessing the performance of the final 
#chosen model on new data. The test data are not used to compare models, or to
#further tweak the model or improve its fit. (If the test data were used for 
#these purposes, they would play a role in building or selecting the best model,
#and would no longer provide an unbiased assessment of the chosen model's 
#performance with completely new data.)

#############################

##Problem 2.3 Consider the sample from a database of credit applicants in 
##Table 2.5. Comment on the likelihood that it was sampled randomly, and whether
##it is likely to be a useful sample.

#This sample is not selected randomly as we can see from "observation #", that
#there is pattern in the observations chosen for the sample. In particular, 
#every 8th observation from the database was selected for the sample. When we 
#select data with such a pre-decided methodology it might introduce a bias in 
#the selected data set. This is true when the order of the observations in the 
#dataset has some meaning (e.g., chronological order).

#############################

##Problem 2.4 Consider the sample from a bank database shown in Table 2.16; 
##it was selected randomly from a larger database to be the training set. 
##Personal Loan indicates whether a solicitation for a personal loan was
##accepted and is the response variable. A campaign is planned for a similar 
##solicitation in the future and the bank is looking for a model that will 
##identify likely responders. Examine the data carefully and indicate what 
##your next step would be.

#Since there are only 18 records and 9 predictor variables in the sample, the
#next step before building a model is to take a larger sample.  18 records is 
#too few to support a model that considers 9 predictors.  How big a sample? 
#The availability of data, the cost and effort involved in data handling, and 
#software capabilities are the main constraining factors.  Also, it is useful 
#to check the number of available responses, and the response ratio, in the 
#larger database.  If the response ratio is very low, it would be worthwhile 
#to oversample the cases where response is positive (or, in other words, 
#undersample the non-response cases).  Two other issues:
#- Zip code should probably be aggregated at a higher level than 5 digits, 
#  which would likely produce an unmanageable number of predictor variables.  
#  For example, it could be aggregated at the level of the first 3 digits.
#- It also seems that the key information in "Mortgage" is whether the person 
#  has a mortgage, and not so much the level of the mortgage, so some 
#  consideration could be given to converting this to a binary variable.
#See also the 2016 blog post by  Tom Fawcett: 
# http://www.kdnuggets.com/2016/08/learning-from-imbalanced-classes.html/

#############################

##Problem 2.5 Using the concept of overfitting, explain why when a model is 
##fit to training data, zero error with those data is not necessarily good.

#Overfitting occurs when the model captures not only the generalizable pattern
#in the data, but also the error. When we split the data into training and 
#validation sets, we assume that the same pattern (if there is a pattern) 
#exists in both, and that they differ only in the error that they contain. 
#An absurd and false model may fit perfectly (on training data set) if the 
#model has enough complexity. Therefore we may get zero error for such a model 
#using the training dataset. Such a model, however, is not likely to give 
#useful results on the validation data set.

#############################

##Problem 2.6 In fitting a model to classify prospects as purchasers or 
##nonpurchasers, a certain company drew the training data from internal data 
##that include demographic and purchase information. Future data to be 
##classified will be lists purchased from other sources, with demographic (but 
##not purchase) data included. It was found that "refund issued" was a useful 
##predictor in the training data. Why is this not an appropriate variable to 
##include in the model?

#The variable "refund issued" is unknown prior to the actual purchase, and 
#therefore is not useful in a predictive model of future purchase behavior. 
#In fact, "refund issued" can only be present for actual purchases but never 
#for non-purchases. This explains why it was found to be closely related to 
#purchase/non-purchase.

#############################

##Problem 2.7 A dataset has 1000 records and 50 variables with 5% of the values
##missing, spread randomly throughout the records and variables. An analyst 
##decides to remove records with missing values. About how many records would 
##you expect to be removed?

#For a record to have all values present, it must avoid having a missing value 
#(P = 0.95) for each of 50 records. The chance that a given record will escape
#having a missing value for two variables is 0.95 * 0.95 = 0.903. The chance 
#that a given record would escape having a missing value for all 50 records 
#is (0.95)^50 = 0.076945. This implies that 1-0.076944 = 0.9231 (92.31%) of 
#all records will have missing values and would be deleted.

#############################

##Problem 2.8 Normalize the data in Table 2.17, showing calculations.

##TABLE 2.17
##            Age       Income($)
##             25          49000
##             56         156000
##             65          99000
##             32         192000
##             41          39000
##             49          57000

#input/load data
data <- data.frame(Age = c(25, 56, 65, 32, 41, 49), 
                      Income = c(49000, 156000, 99000, 192000, 39000, 57000))
data
#> data
#  Age Income
#1  25  49000
#2  56 156000
#3  65  99000
#4  32 192000
#5  41  39000
#6  49  57000

#normalize data
data.norm <- scale(data)
data.norm
#> data.norm
#Age       Income
#[1,] -1.3132530 -0.790026927
#[2,]  0.7567898  0.911977392
#[3,]  1.3577700  0.005302194
#[4,] -0.8458239  1.484614360
#[5,] -0.2448438 -0.949092751
#[6,]  0.2893608 -0.662774268
#attr(,"scaled:center")
#Age      Income 
#44.66667 98666.66667 
#attr(,"scaled:scale")
#Age      Income 
#14.97554 62867.05550

#############################

##Problem 2.9 Statistical distance between records can be measured in several
##ways. Consider Euclidean distance, measured as the square root of the sum of 
##the squared differences.
##For the first two records in Table 2.17, it is
##sqrt((25-56)^2 + (49000 - 156000)^2)

##Can normalizing the data change which two records are farthest from each
##other in terms of Euclidean distance?

#Yes, it can. By normalizing we equate the scales of the different variables,
#and therefore the Euclidean distance can dramatically change. In the example 
#age is in single years while income is in thousands of dollars. The Euclidean 
#distance on the raw data is therefore almost completely determined by income 
#and unaffected by age. In contrast, after normalizing age and income will be
#on the same scale. Age will then have a much larger impact on the Euclidean 
#distance. To see this in practice, examine the table below that compare the
#Euclidean distance before and after normalizing the data.

#before normalizing
euclidean.dist <- dist(data, method = "euclidean")
euclidean.dist
#> euclidean.dist
#           1          2          3          4          5
#2 107000.004                                            
#3  50000.016  57000.001                                 
#4 143000.000  36000.008  93000.006                      
#5  10000.013 117000.001  60000.005 153000.000           
#6   8000.036  99000.000  42000.003 135000.001  18000.002

#after normalizing
euclidean.dist.norm <- dist(data.norm, method = "euclidean")
euclidean.dist.norm
#> euclidean.dist.norm
#          1         2         3         4         5
#2 2.6799060                                        
#3 2.7869181 1.0877670                              
#4 2.3221720 1.7018473 2.6540895                    
#5 1.0801852 2.1134928 1.8652723 2.5068122          
#6 1.6076580 1.6426602 1.2600890 2.4289756 0.6060964

#We now see that records #4 and #5 (distance = 153000) are farthest from 
#each other before normalizing, whereas records #1 and #3 are farthest from 
#each other after normalizing.

#############################

##Problem 2.10 Two models are applied to a dataset that has been partitioned. 
##Model A is considerably more accurate than model B on the training data, but
##slightly less accurate than model B on the validation data. Which model are
##you more likely to consider for final deployment?

#We prefer the model with the lowest error on the validation data. Model A 
#might be overfitting the training data. We would therefore select model B 
#for deployment on new data.

#############################

##Problem 2.11 The dataset ToyotaCorolla.csv contains data on used cars on sale
##during the late summer of 2004 in the Netherlands. It has 1436 records 
##containing details on 38 attributes, including Price, Age, Kilometers, HP, 
##and other specifications.
##We plan to analyze the data using various data mining techniques described in
##future chapters. Prepare the data for use as follows:

##2.11.a. The dataset has two categorical attributes, Fuel Type and Color. 
##Describe how you would convert these to binary variables. Confirm this using 
##R's functions to transform categorical data into dummies. What would you do
##with the variable Model?

#NOTE: Metallic is binary and not needed to be converted to dummies.

#The categorical fuel_type variable has three categories: petrol, diesel and 
#CNG. The binary variable Petrol gets the value 1 if Fuel Type=Petrol and 
#otherwise it gets the value 0. The binary variable Diesel gets the value 1 if
#Fuel Type=Diesel and otherwise it gets the value 0. If Fuel type is CNG, both 
#binary variables Type=Petrol and Type=Diesel take the value 0.

#Similarly, the variable Color is converted to dummy variables. It resulted in 
#10 dummy variables with Color_Beige" as the reference category.

#Making dummies of all the categories in Model would make for a lot of 
#predictor variables.  Some preliminary exploration should be done first, to 
#see if there are a small number of models that account for most cases.  
#The analysis could be confined initially to those to see how important Model 
#is as a predictor.


#install.packages("dummies")
library(dummies)

#load data
car.df <- read.csv("ToyotaCorolla.csv")
#create binary dummy variables for Fuel_Type
car.df <- dummy.data.frame(car.df, names = c("Fuel_Type", "Color"), 
                           sep=".")
dim(car.df)
t(t(names(car.df)))
head(car.df[,8:10], 10)
#> head(car.df[,8:10], 10)
#   Fuel_Type.CNG Fuel_Type.Diesel Fuel_Type.Petrol
#1              0                1                0
#2              0                1                0
#3              0                1                0
#4              0                1                0
#5              0                1                0
#6              0                1                0
#7              0                1                0
#8              0                1                0
#9              0                0                1
#10             0                1                0

head(car.df[,13:22], 10)
#> head(car.df[,13:22], 10)
#   Color.Beige Color.Black Color.Blue Color.Green Color.Grey Color.Red Color.Silver
#1            0           0          1           0          0         0            0
#2            0           0          0           0          0         0            1
#3            0           0          1           0          0         0            0
#4            0           1          0           0          0         0            0
#5            0           1          0           0          0         0            0
#6            0           0          0           0          0         0            0
#7            0           0          0           0          1         0            0
#8            0           0          0           0          1         0            0
#9            0           0          0           0          0         1            0
#10           0           0          1           0          0         0            0
#   Color.Violet Color.White Color.Yellow
#1             0           0            0
#2             0           0            0
#3             0           0            0
#4             0           0            0
#5             0           0            0
#6             0           1            0
#7             0           0            0
#8             0           0            0
#9             0           0            0
#10            0           0            0

##2.1.b. Prepare the dataset (as factored into dummies) for data mining 
##techniques of supervised learning by creating partitions in R. Select  
##the variables and use default values for the random seed and partitioning
##percentages for training (50%), validation (30%), and test (20%) sets. 
##Describe the roles that these partitions will play in modeling.

#Training dataset
#The training dataset is used to train or build models. For example, in a 
#linear regression, the training dataset is used to fit the linear regression 
#model, i.e. to compute the regression coefficients. This is usually the 
#largest partition.

#Validation dataset
#Once a model is built on training data, we assess the accuracy of the model on
#unseen data. For this, the model should be used on a dataset that was not used
#in the training process. In the validation data we know the actual value of 
#the response variable, and can therefore examine the difference between the 
#actual value and the predicted value to determine the error in prediction. 
#Based on this performance, sometimes the validation dataset is used to tweak 
#the model, or to choose between multiple fitted models.

#Test dataset
#The validation dataset is often used to select a model with minimum error. 
#Testing that model on completely unseen data gives a realistic estimate of 
#the performance of the model. When a model is finally chosen, its accuracy
#with the validation dataset is still an optimistic estimate of how it would 
#perform with unseen data. This is because (1) the final model has come out as 
#the winner among the competing models based on the fact that its accuracy with
#the validation dataset is highest, and/or (2) the validation set was used to 
#help build one or more models. Thus, you need to set aside yet another portion 
#of data, which is used neither in training nor in validation, which is called 
#the test dataset. The accuracy of the model on the test data gives a realistic
#estimate of the performance of the model on completely unseen data.

#use set.seed() to get the same partitions when re-running the R code.
set.seed(201)

#partitioning into training (60%) and validation (40%)
#randomly sample 60% of the row IDs for training; the remaining 40% serve as
#validation
train.rows <- sample(rownames(car.df), dim(car.df)[1]*0.6)
# collect all the columns with training row ID into training set:
train.data <- car.df[train.rows, ]
# assign row IDs that are not already in the training set, into validation
valid.rows <- setdiff(rownames(car.df), train.rows)
valid.data <- car.df[valid.rows, ]

## partitioning into training (50%), validation (30%), test (20%)
# randomly sample 50% of the row IDs for training
train.rows <- sample(rownames(car.df), dim(car.df)[1]*0.5)
# sample 30% of the row IDs into the validation set, drawing only from records
# not already in the training set
# use setdiff() to find records not already in the training set
valid.rows <- sample(setdiff(rownames(car.df), train.rows), dim(car.df)[1]*0.3)
# assign the remaining 20% row IDs serve as test
test.rows <- setdiff(rownames(car.df), union(train.rows, valid.rows))
# create the 3 data frames by collecting all columns from the appropriate rows
train.data <- car.df[train.rows, ]
valid.data <- car.df[valid.rows, ]
test.data <- car.df[test.rows, ]
dim(car.df)
dim(train.data)
dim(valid.data)
dim(test.data)
